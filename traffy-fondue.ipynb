{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: folium in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (2.28.1)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (1.21.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (2022.12.7)\n",
      "Requirement already satisfied: geopy in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "! pip install findspark\n",
    "! pip install folium\n",
    "! pip install geopy\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, regexp_replace, split, col, size, array_contains, isnan, when, count, array, reverse, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import kurtosis, skew\n",
    "import math\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'\n",
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark Project')\\\n",
    "        .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.csv.parser.multiLine\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+--------------------+--------------------+--------------+----+------------+--------------------+\n",
      "|  ticket_id|               type|        organization|             comment|               photo|         photo_after|            coords|             address|subdistrict|district|            province|           timestamp|         state|star|count_reopen|       last_activity|\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+--------------------+--------------------+--------------+----+------------+--------------------+\n",
      "|2021-9LHDM6|                 {}|                null|            ไม่มีภาพ|https://storage.g...|                null|100.48661,13.79386|1867 จรัญสนิทวงศ์...|    บางพลัด| บางพลัด|       กรุงเทพมหานคร|2021-09-01 10:44:...|กำลังดำเนินการ|null|        null|2022-02-22 04:59:...|\n",
      "|2021-FYJTFP|        {ความสะอาด}|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|                null|100.53084,13.81865|12/14 ถนน กรุงเทพ...|       null|    null|       กรุงเทพมหานคร|2021-09-03 12:51:...|     เสร็จสิ้น|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|            {สายไฟ}|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|                null|100.57685,13.79704|335/31 ลาดพร้าว แ...|  สามเสนนอก|ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|กำลังดำเนินการ|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|        {ถนน,สะพาน}|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|100.52916,13.72338|37 10 ซอย สีลม 9 ...|       สีลม|  บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|กำลังดำเนินการ|null|        null|2022-02-22 04:30:...|\n",
      "|2021-CGPMUN|{น้ำท่วม,ร้องเรียน}|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|100.66709,13.67891|189 เฉลิมพระเกียร...|    หนองบอน|  ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|     เสร็จสิ้น|   4|        null|2022-06-21 08:21:...|\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+--------------------+--------------------+--------------+----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'bangkok_traffy.csv'\n",
    "df = spark.read.option(\"multiLine\", \"true\").csv(path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+------+------------+-------------+\n",
      "|ticket_id|type|organization|comment|photo|photo_after|coords|address|subdistrict|district|province|timestamp|state|  star|count_reopen|last_activity|\n",
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+------+------------+-------------+\n",
      "|     2413|1550|        2640|   3911| 2089|      85624|  2019|   4433|       2089|    2092|    2393|     2233| 2026|164087|      120042|         2558|\n",
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ticket_id=None, type='{ถนน}', organization='สำนักงาน ป.ป.ท.,เขตจอมทอง,ฝ่ายเทศกิจ เขตจอมทอง,ผอ.เขตจอมทอง (นายณัฐพงษ์),กลุ่มกรุงธนเหนือ (รองปลัดฯ เฉลิมพล)', comment=None, photo='https://storage.googleapis.com/traffy_public_bucket/TeamChadChart/corruption_photo2.png', photo_after='https://storage.googleapis.com/traffy_public_bucket/attachment/2022-06/e9596093de70ae8abacd6574f26a2d0f4466fe9f.jpg', coords='100.45568,13.69103', address=None, subdistrict='บางขุนเทียน', district='จอมทอง', province='กรุงเทพมหานคร', timestamp='2022-06-09 23:34:34.98044+00', state='เสร็จสิ้น', star='5', count_reopen=None, last_activity='2022-06-10 11:02:34.607728+00') \n",
      "\n",
      "Row(ticket_id='2022-7DABXT', type='{สะพาน}', organization=None, comment='\"เคยดีใจมีสายสีน้ำเงินสถานี\"\"แยกไฟฉาย\"\"', photo=None, photo_after=None, coords=None, address=None, subdistrict=None, district=None, province=None, timestamp=None, state=None, star=None, count_reopen=None, last_activity=None) \n",
      "\n",
      "Row(ticket_id='2022-7DABXT', type='{สะพาน}', organization=None, comment='\"เคยดีใจมีสายสีน้ำเงินสถานี\"\"แยกไฟฉาย\"\"', photo=None, photo_after=None, coords=None, address=None, subdistrict=None, district=None, province=None, timestamp=None, state=None, star=None, count_reopen=None, last_activity=None) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only rows with null values in the \"column_name\" column\n",
    "def check_first_null(filtered_df):\n",
    "    # Check if the filtered DataFrame is empty\n",
    "    if filtered_df.count() == 0:\n",
    "        return f\"No null values in {filtered_df}.\"\n",
    "    else:\n",
    "        first_row = filtered_df.head()\n",
    "        return first_row\n",
    "    \n",
    "ticket_id_null_df = df.filter(df.ticket_id.isNull())\n",
    "coords_null_df = df.filter(df.coords.isNull())\n",
    "address_null_df = df.filter(df.address.isNull())\n",
    "print(check_first_null(ticket_id_null_df), \"\\n\")\n",
    "print(check_first_null(coords_null_df), \"\\n\")\n",
    "print(check_first_null(address_null_df), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above cells give us the first row with null value from each column selected (ticket_id, coords, address) to tell some relationship of those null value.\n",
    "1. The ticket_id is null when the state='เสร็จสิ้น', it's mean we can drop this column significantly.\n",
    "2. The address has null value 2 times more than coords. In the first null row we can see both of them are null. So it might tell that if no coords, no address too and not vice versa. We'll check in next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|coords|address|\n",
      "+------+-------+\n",
      "|     1|   2415|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub = ['coords', 'address']\n",
    "df = df.dropna(how='all', subset=sub)\n",
    "sub_null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sub])\n",
    "sub_null_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the null coords most dissapear. So, we don't need to map any address to coords and we can also drop all remained null column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all', subset=['coords'])\n",
    "df = df.dropna(how='all', subset=['address'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this step, we'll use df_use to be a data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|  ticket_id|                type|        organization|             comment|               photo|         photo_after|              coords|             address|subdistrict|district|            province|           timestamp|state|star|count_reopen|       last_activity|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|2021-9LHDM6|                  []|                null|            ไม่มีภาพ|https://storage.g...|                null|[13.79386, 100.48...|1867 จรัญสนิทวงศ์...|    บางพลัด| บางพลัด|       กรุงเทพมหานคร|2021-09-01 10:44:...|    0|null|        null|2022-02-22 04:59:...|\n",
      "|2021-FYJTFP|         [ความสะอาด]|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|                null|[13.81865, 100.53...|12/14 ถนน กรุงเทพ...|       null|    null|       กรุงเทพมหานคร|2021-09-03 12:51:...|    1|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|             [สายไฟ]|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|                null|[13.79704, 100.57...|335/31 ลาดพร้าว แ...|  สามเสนนอก|ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|        [ถนน, สะพาน]|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|[13.72338, 100.52...|37 10 ซอย สีลม 9 ...|       สีลม|  บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-CGPMUN|[น้ำท่วม, ร้องเรียน]|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|[13.67891, 100.66...|189 เฉลิมพระเกียร...|    หนองบอน|  ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|    1|   4|        null|2022-06-21 08:21:...|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change state to boolean \n",
    "df_use = df.withColumn('state', when(df.state == 'เสร็จสิ้น', 1).otherwise(0))\n",
    "\n",
    "# change type to list\n",
    "df_use = df_use.withColumn(\"type\", split(regexp_replace(\"type\", \"[{}]\", \"\"), \",\"))\n",
    "df_use = df_use.dropna(how='all', subset=['type'])\n",
    "\n",
    "# change coords to pair and swap them into format [latitude, longtitude]\n",
    "flatten = udf(lambda x: list(chain.from_iterable(x)), ArrayType(StringType()))\n",
    "df_use = df_use.withColumn('coords', array(reverse(split(df.coords, ','))))\n",
    "df_use = df_use.withColumn('coords', flatten('coords'))\n",
    "df_use.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|  ticket_id|       type|        organization|             comment|               photo|         photo_after|              coords|             address|   subdistrict| district|            province|           timestamp|state|star|count_reopen|       last_activity|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|2021-FYJTFP|  ความสะอาด|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|                null|[13.81865, 100.53...|12/14 ถนน กรุงเทพ...|          null|     null|       กรุงเทพมหานคร|2021-09-03 12:51:...|    1|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|      สายไฟ|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|                null|[13.79704, 100.57...|335/31 ลาดพร้าว แ...|     สามเสนนอก| ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|        ถนน|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|[13.72338, 100.52...|37 10 ซอย สีลม 9 ...|          สีลม|   บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|      สะพาน|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|[13.72338, 100.52...|37 10 ซอย สีลม 9 ...|          สีลม|   บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-CGPMUN|    น้ำท่วม|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|[13.67891, 100.66...|189 เฉลิมพระเกียร...|       หนองบอน|   ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|    1|   4|        null|2022-06-21 08:21:...|\n",
      "|2021-CGPMUN|  ร้องเรียน|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|[13.67891, 100.66...|189 เฉลิมพระเกียร...|       หนองบอน|   ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|    1|   4|        null|2022-06-21 08:21:...|\n",
      "|2021-8Z8JQ3|        ถนน|                null|1) พื้นที่นี้ขาดห...|https://storage.g...|                null|[13.72804, 100.86...|3/8 ขุมทอง-ลำต้อย...|        ขุมทอง|ลาดกระบัง|       กรุงเทพมหานคร|2021-09-20 00:24:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-8Z8JQ3|  ร้องเรียน|                null|1) พื้นที่นี้ขาดห...|https://storage.g...|                null|[13.72804, 100.86...|3/8 ขุมทอง-ลำต้อย...|        ขุมทอง|ลาดกระบัง|       กรุงเทพมหานคร|2021-09-20 00:24:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-8Z8JQ3|    น้ำท่วม|                null|1) พื้นที่นี้ขาดห...|https://storage.g...|                null|[13.72804, 100.86...|3/8 ขุมทอง-ลำต้อย...|        ขุมทอง|ลาดกระบัง|       กรุงเทพมหานคร|2021-09-20 00:24:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-8Z8JQ3|ความปลอดภัย|                null|1) พื้นที่นี้ขาดห...|https://storage.g...|                null|[13.72804, 100.86...|3/8 ขุมทอง-ลำต้อย...|        ขุมทอง|ลาดกระบัง|       กรุงเทพมหานคร|2021-09-20 00:24:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-G4FN8W|       คลอง|                null|   น้ำเน่าเสียในคลอง|https://storage.g...|                null|[13.74227, 100.62...|50 พระราม ที่ 9 แ...|       สวนหลวง|  สวนหลวง|       กรุงเทพมหานคร|2021-09-21 05:22:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-AFJYNF|        ถนน|                null|น้ำท่วม เอ่อล้นถน...|https://storage.g...|                null|[13.73614, 100.71...|3 ถนน พัฒนาชนบท 3...|คลองสองต้นนุ่น|ลาดกระบัง|       กรุงเทพมหานคร|2021-09-22 05:06:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-AFJYNF|    น้ำท่วม|                null|น้ำท่วม เอ่อล้นถน...|https://storage.g...|                null|[13.73614, 100.71...|3 ถนน พัฒนาชนบท 3...|คลองสองต้นนุ่น|ลาดกระบัง|       กรุงเทพมหานคร|2021-09-22 05:06:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-HAJULK|  ร้องเรียน|                null|มีการยกถนนในซอยเม...|https://storage.g...|                null|[13.79225, 100.71...|95 ซอย 12 มีนบุรี...|       มีนบุรี|  มีนบุรี|       กรุงเทพมหานคร|2021-09-23 06:25:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-HAJULK|        ถนน|                null|มีการยกถนนในซอยเม...|https://storage.g...|                null|[13.79225, 100.71...|95 ซอย 12 มีนบุรี...|       มีนบุรี|  มีนบุรี|       กรุงเทพมหานคร|2021-09-23 06:25:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-HAJULK|    น้ำท่วม|                null|มีการยกถนนในซอยเม...|https://storage.g...|                null|[13.79225, 100.71...|95 ซอย 12 มีนบุรี...|       มีนบุรี|  มีนบุรี|       กรุงเทพมหานคร|2021-09-23 06:25:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-7XATFA|      สะพาน|             เขตสาทร|สะพานลอยปรับปรุงไ...|https://storage.g...|                null|[13.72060, 100.52...|191/1 ถนน สาทรเหน...|       ยานนาวา|     สาทร|       กรุงเทพมหานคร|2021-09-26 05:03:...|    1|null|        null|2022-06-06 01:17:...|\n",
      "|2021-79YL3M|        ถนน|                null|1.ซอยราษฎร์นิมิตร...|https://storage.g...|                null|[13.91732, 100.73...|147 ซอย ราษฎร์นิม...| สามวาตะวันออก|คลองสามวา|       กรุงเทพมหานคร|2021-09-28 23:01:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-9U2NJT|    น้ำท่วม|เขตบางซื่อ,ฝ่ายโย...|             น้ำท่วม|https://storage.g...|https://storage.g...|[13.81853, 100.53...|12/14 ถนน กรุงเทพ...|          null|     null|       กรุงเทพมหานคร|2021-10-14 10:45:...|    1|null|        null|2022-09-08 08:35:...|\n",
      "|2021-F7A2PK|ท่อระบายน้ำ|                null|มีฝาท่อระบายน้ำชำ...|https://storage.g...|                null|[13.71937, 100.44...|19/31 ซอย เพชรเกษ...|       บางหว้า|ภาษีเจริญ|       กรุงเทพมหานคร|2021-11-12 02:19:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exploded = df_use.withColumn(\"type\", explode(\"type\"))\n",
    "df_exploded = df_exploded.filter(\"type != ''\")\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_exploded.withColumn(\"latitude\", col(\"coords\")[0]) \\\n",
    "       .withColumn(\"longitude\", col(\"coords\")[1]) \\\n",
    "       .drop(\"coords\")\n",
    "df_t = df_t.withColumn(\"latitude\", col(\"latitude\").cast('double'))\n",
    "df_t = df_t.withColumn(\"longitude\", col(\"longitude\").cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ticket_id='จักขอบคุณอย่างยิ่ง\"', type='https://storage.googleapis.com/traffy_public_bucket/attachment/2022-02/5d20ecb864e916af948b4090ce9405be6f0b2f04.jpg', organization=None, comment='100.46930,13.75503', photo='1186 ถ. พรานนก แขวง บ้านช่างหล่อ เขตบางกอกน้อย กรุงเทพมหานคร 10700 ประเทศไทย', photo_after='บางขุนศรี', address='กรุงเทพมหานคร', subdistrict='2022-02-06 16:59:19.251528+00', district='กำลังดำเนินการ', province=None, timestamp=None, state=0, star=None, count_reopen=None, last_activity=None, latitude=None, longitude=None) \n",
      "\n",
      "Row(ticket_id='จักขอบคุณอย่างยิ่ง\"', type='https://storage.googleapis.com/traffy_public_bucket/attachment/2022-02/5d20ecb864e916af948b4090ce9405be6f0b2f04.jpg', organization=None, comment='100.46930,13.75503', photo='1186 ถ. พรานนก แขวง บ้านช่างหล่อ เขตบางกอกน้อย กรุงเทพมหานคร 10700 ประเทศไทย', photo_after='บางขุนศรี', address='กรุงเทพมหานคร', subdistrict='2022-02-06 16:59:19.251528+00', district='กำลังดำเนินการ', province=None, timestamp=None, state=0, star=None, count_reopen=None, last_activity=None, latitude=None, longitude=None) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_latitude = df_t.filter(df_t.latitude.isNull())\n",
    "null_longitude = df_t.filter(df_t.longitude.isNull())\n",
    "print(check_first_null(null_latitude), \"\\n\")\n",
    "print(check_first_null(null_longitude), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_t.dropna(how='all', subset=['latitude'])\n",
    "df_t = df_t.dropna(how='all', subset=['longitude'])\n",
    "df_t = df_t.dropna(how='all', subset=['subdistrict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25176456585133605\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "type_indexer = StringIndexer(inputCol='type', outputCol='label')\n",
    "df_indexed = type_indexer.fit(df_t).transform(df_t)\n",
    "\n",
    "# Create a feature vector\n",
    "assembler = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features')\n",
    "df_features = assembler.transform(df_indexed)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = df_features.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf = RandomForestClassifier(numTrees=100, maxDepth=5, labelCol='label', featuresCol='features')\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Evaluate performance on test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "predictions = rf_model.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_6f8215f805b0,\n",
       " OneHotEncoder_bb3d82c4692f,\n",
       " VectorAssembler_737f6f55c8fa]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "categoricalAttributes = ['type']\n",
    "for columnName in categoricalAttributes:\n",
    "    stringIndexer = StringIndexer(inputCol=columnName, outputCol=columnName+ \"Index\")\n",
    "    stages.append(stringIndexer)\n",
    "    oneHotEncoder = OneHotEncoder(inputCol=columnName+ \"Index\", outputCol=columnName + \"Vec\")\n",
    "    stages.append(oneHotEncoder)\n",
    "    \n",
    "categoricalCols = [s + \"Vec\" for s in categoricalAttributes]\n",
    "\n",
    "numericColumns = ['latitude', 'longitude']\n",
    "\n",
    "allFeatureCols = numericColumns + categoricalCols\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=allFeatureCols,\n",
    "    outputCol=\"features\")\n",
    "stages.append(vectorAssembler)\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_t.select(['type', 'subdistrict', 'latitude', 'longitude'])\n",
    "train_df, test_df = df_select.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurePipeline = Pipeline(stages=stages)\n",
    "featureOnlyModel = featurePipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+---------+---------+---------------+--------------------+\n",
      "| type|subdistrict|latitude|longitude|typeIndex|        typeVec|            features|\n",
      "+-----+-----------+--------+---------+---------+---------------+--------------------+\n",
      "|PM2.5| กระทุ่มราย|13.85535|100.86528|     17.0|(23,[17],[1.0])|(25,[0,1,19],[13....|\n",
      "+-----+-----------+--------+---------+---------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingFeaturesDf = featureOnlyModel.transform(train_df)\n",
    "testFeaturesDf = featureOnlyModel.transform(test_df)\n",
    "trainingFeaturesDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(25, {0: 13.8554, 1: 100.8653, 19: 1.0}), latitude=13.85535, longitude=100.86528),\n",
       " Row(features=SparseVector(25, {0: 13.8563, 1: 100.863, 19: 1.0}), latitude=13.85633, longitude=100.86304),\n",
       " Row(features=SparseVector(25, {0: 13.6974, 1: 100.8515, 19: 1.0}), latitude=13.69736, longitude=100.8515),\n",
       " Row(features=SparseVector(25, {0: 13.7018, 1: 100.8542, 19: 1.0}), latitude=13.70184, longitude=100.85423),\n",
       " Row(features=SparseVector(25, {0: 13.7547, 1: 100.8514, 19: 1.0}), latitude=13.75473, longitude=100.85137)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingFeaturesDf.select(\"features\", 'latitude', 'longitude').rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=100, maxDepth=5, labelCol='subdistrict', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol='type', featuresCol='features')\n",
    "dtPipeline = Pipeline(stages=[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column type must be of type numeric but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17916\\791832439.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdtPipelineModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtPipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingFeaturesDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column type must be of type numeric but was actually of type string."
     ]
    }
   ],
   "source": [
    "dtPipelineModel = dtPipeline.fit(trainingFeaturesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.252609653038148\n"
     ]
    }
   ],
   "source": [
    "df_select = df_t.select(['type', 'latitude', 'longitude'])\n",
    "\n",
    "indexer = StringIndexer(inputCol='type', outputCol='label')\n",
    "assembler = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features')\n",
    "pipeline = Pipeline(stages=[indexer, assembler])\n",
    "preprocessed_data = pipeline.fit(df_select).transform(df_select)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features', maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 0 has 336 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22180\\2479537638.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Train Decision Tree classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdt_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Make predictions on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 0 has 336 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "df_select = df_t.select(['type', 'subdistrict', 'latitude', 'longitude'])\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"type\", outputCol=\"label\").fit(df_select)\n",
    "df_select = label_indexer.transform(df_select)\n",
    "feature_indexer = StringIndexer(inputCol=\"subdistrict\", outputCol=\"subdistrict\" + \"_index\")\n",
    "oneHotEncoder = OneHotEncoder(inputCol=\"subdistrict\" + \"_index\", outputCol=\"subdistrict\" + \"_vec\")\n",
    "df_select = feature_indexer.transform(df_select)\n",
    "# assembler = VectorAssembler(inputCols=['indexfeatures'], outputCol='features')\n",
    "# df_select = assembler.transform(df_select)\n",
    "\n",
    "pipeline = Pipeline(stages=[label_indexer, feature_indexer])\n",
    "preprocessed_data = pipeline.fit(df_select).transform(df_select)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = df_select.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features', maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Python 3.10.9)",
   "language": "python",
   "name": "py3109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
