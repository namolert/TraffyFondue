{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: folium in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (2.28.1)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from folium) (1.21.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from requests->folium) (2022.12.7)\n",
      "Requirement already satisfied: geopy in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\acer\\miniconda3\\envs\\py3109\\lib\\site-packages (from scikit-learn) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "! pip install findspark\n",
    "! pip install folium\n",
    "! pip install geopy\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, regexp_replace, split, col, size, array_contains, isnan, when, count, array, reverse, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import kurtosis, skew\n",
    "import math\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'\n",
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark Project')\\\n",
    "        .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.csv.parser.multiLine\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+--------------------+--------------------+--------------+----+------------+--------------------+\n",
      "|  ticket_id|               type|        organization|             comment|               photo|         photo_after|            coords|             address|subdistrict|district|            province|           timestamp|         state|star|count_reopen|       last_activity|\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+--------------------+--------------------+--------------+----+------------+--------------------+\n",
      "|2021-9LHDM6|                 {}|                null|            ไม่มีภาพ|https://storage.g...|                null|100.48661,13.79386|1867 จรัญสนิทวงศ์...|    บางพลัด| บางพลัด|       กรุงเทพมหานคร|2021-09-01 10:44:...|กำลังดำเนินการ|null|        null|2022-02-22 04:59:...|\n",
      "|2021-FYJTFP|        {ความสะอาด}|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|                null|100.53084,13.81865|12/14 ถนน กรุงเทพ...|       null|    null|       กรุงเทพมหานคร|2021-09-03 12:51:...|     เสร็จสิ้น|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|            {สายไฟ}|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|                null|100.57685,13.79704|335/31 ลาดพร้าว แ...|  สามเสนนอก|ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|กำลังดำเนินการ|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|        {ถนน,สะพาน}|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|100.52916,13.72338|37 10 ซอย สีลม 9 ...|       สีลม|  บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|กำลังดำเนินการ|null|        null|2022-02-22 04:30:...|\n",
      "|2021-CGPMUN|{น้ำท่วม,ร้องเรียน}|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|100.66709,13.67891|189 เฉลิมพระเกียร...|    หนองบอน|  ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|     เสร็จสิ้น|   4|        null|2022-06-21 08:21:...|\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+--------------------+--------------------+--------------+----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'bangkok_traffy.csv'\n",
    "df = spark.read.option(\"multiLine\", \"true\").csv(path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+------+------------+-------------+\n",
      "|ticket_id|type|organization|comment|photo|photo_after|coords|address|subdistrict|district|province|timestamp|state|  star|count_reopen|last_activity|\n",
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+------+------------+-------------+\n",
      "|     2413|1550|        2640|   3911| 2089|      85624|  2019|   4433|       2089|    2092|    2393|     2233| 2026|164087|      120042|         2558|\n",
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ticket_id=None, type='{ถนน}', organization='สำนักงาน ป.ป.ท.,เขตจอมทอง,ฝ่ายเทศกิจ เขตจอมทอง,ผอ.เขตจอมทอง (นายณัฐพงษ์),กลุ่มกรุงธนเหนือ (รองปลัดฯ เฉลิมพล)', comment=None, photo='https://storage.googleapis.com/traffy_public_bucket/TeamChadChart/corruption_photo2.png', photo_after='https://storage.googleapis.com/traffy_public_bucket/attachment/2022-06/e9596093de70ae8abacd6574f26a2d0f4466fe9f.jpg', coords='100.45568,13.69103', address=None, subdistrict='บางขุนเทียน', district='จอมทอง', province='กรุงเทพมหานคร', timestamp='2022-06-09 23:34:34.98044+00', state='เสร็จสิ้น', star='5', count_reopen=None, last_activity='2022-06-10 11:02:34.607728+00') \n",
      "\n",
      "Row(ticket_id='2022-7DABXT', type='{สะพาน}', organization=None, comment='\"เคยดีใจมีสายสีน้ำเงินสถานี\"\"แยกไฟฉาย\"\"', photo=None, photo_after=None, coords=None, address=None, subdistrict=None, district=None, province=None, timestamp=None, state=None, star=None, count_reopen=None, last_activity=None) \n",
      "\n",
      "Row(ticket_id='2022-7DABXT', type='{สะพาน}', organization=None, comment='\"เคยดีใจมีสายสีน้ำเงินสถานี\"\"แยกไฟฉาย\"\"', photo=None, photo_after=None, coords=None, address=None, subdistrict=None, district=None, province=None, timestamp=None, state=None, star=None, count_reopen=None, last_activity=None) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only rows with null values in the \"column_name\" column\n",
    "def check_first_null(filtered_df):\n",
    "    # Check if the filtered DataFrame is empty\n",
    "    if filtered_df.count() == 0:\n",
    "        return f\"No null values in {filtered_df}.\"\n",
    "    else:\n",
    "        first_row = filtered_df.head()\n",
    "        return first_row\n",
    "    \n",
    "ticket_id_null_df = df.filter(df.ticket_id.isNull())\n",
    "coords_null_df = df.filter(df.coords.isNull())\n",
    "address_null_df = df.filter(df.address.isNull())\n",
    "print(check_first_null(ticket_id_null_df), \"\\n\")\n",
    "print(check_first_null(coords_null_df), \"\\n\")\n",
    "print(check_first_null(address_null_df), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above cells give us the first row with null value from each column selected (ticket_id, coords, address) to tell some relationship of those null value.\n",
    "1. The ticket_id is null when the state='เสร็จสิ้น', it's mean we can drop this column significantly.\n",
    "2. The address has null value 2 times more than coords. In the first null row we can see both of them are null. So it might tell that if no coords, no address too and not vice versa. We'll check in next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|coords|address|\n",
      "+------+-------+\n",
      "|     1|   2415|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub = ['coords', 'address']\n",
    "df = df.dropna(how='all', subset=sub)\n",
    "sub_null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sub])\n",
    "sub_null_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the null coords most dissapear. So, we don't need to map any address to coords and we can also drop all remained null column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all', subset=['coords'])\n",
    "df = df.dropna(how='all', subset=['address'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this step, we'll use df_use to be a data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|  ticket_id|                type|        organization|             comment|               photo|         photo_after|              coords|             address|subdistrict|district|            province|           timestamp|state|star|count_reopen|       last_activity|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|2021-9LHDM6|                  []|                null|            ไม่มีภาพ|https://storage.g...|                null|[13.79386, 100.48...|1867 จรัญสนิทวงศ์...|    บางพลัด| บางพลัด|       กรุงเทพมหานคร|2021-09-01 10:44:...|    0|null|        null|2022-02-22 04:59:...|\n",
      "|2021-FYJTFP|         [ความสะอาด]|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|                null|[13.81865, 100.53...|12/14 ถนน กรุงเทพ...|       null|    null|       กรุงเทพมหานคร|2021-09-03 12:51:...|    1|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|             [สายไฟ]|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|                null|[13.79704, 100.57...|335/31 ลาดพร้าว แ...|  สามเสนนอก|ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|        [ถนน, สะพาน]|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|[13.72338, 100.52...|37 10 ซอย สีลม 9 ...|       สีลม|  บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-CGPMUN|[น้ำท่วม, ร้องเรียน]|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|[13.67891, 100.66...|189 เฉลิมพระเกียร...|    หนองบอน|  ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|    1|   4|        null|2022-06-21 08:21:...|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change state to boolean \n",
    "df_use = df.withColumn('state', when(df.state == 'เสร็จสิ้น', 1).otherwise(0))\n",
    "\n",
    "# change type to list\n",
    "df_use = df_use.withColumn(\"type\", split(regexp_replace(\"type\", \"[{}]\", \"\"), \",\"))\n",
    "df_use = df_use.dropna(how='all', subset=['type'])\n",
    "\n",
    "# change coords to pair and swap them into format [latitude, longtitude]\n",
    "flatten = udf(lambda x: list(chain.from_iterable(x)), ArrayType(StringType()))\n",
    "df_use = df_use.withColumn('coords', array(reverse(split(df.coords, ','))))\n",
    "df_use = df_use.withColumn('coords', flatten('coords'))\n",
    "df_use.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|  ticket_id|       type|        organization|             comment|               photo|photo_after|              coords|             address|subdistrict|district|            province|           timestamp|state|star|count_reopen|       last_activity|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|2021-9LHDM6|         []|                null|            ไม่มีภาพ|https://storage.g...|       null|[13.79386, 100.48...|1867 จรัญสนิทวงศ์...|    บางพลัด| บางพลัด|       กรุงเทพมหานคร|2021-09-01 10:44:...|    0|null|        null|2022-02-22 04:59:...|\n",
      "|2021-FYJTFP|[ความสะอาด]|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|       null|[13.81865, 100.53...|12/14 ถนน กรุงเทพ...|       null|    null|       กรุงเทพมหานคร|2021-09-03 12:51:...|    1|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|    [สายไฟ]|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|       null|[13.79704, 100.57...|335/31 ลาดพร้าว แ...|  สามเสนนอก|ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-G4FN8W|     [คลอง]|                null|   น้ำเน่าเสียในคลอง|https://storage.g...|       null|[13.74227, 100.62...|50 พระราม ที่ 9 แ...|    สวนหลวง| สวนหลวง|       กรุงเทพมหานคร|2021-09-21 05:22:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-Q7XMCN|         []|                null|งานไม่เสร็จ ควรเก...|https://storage.g...|       null|[13.79675, 100.50...|1 ถนน จรัญสนิทวงศ...|     บางอ้อ| บางพลัด|       กรุงเทพมหานคร|2021-09-24 04:15:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------+--------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ex = df_use.filter(size(col('type')) == 1)\n",
    "df_ex.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|  ticket_id|       type|        organization|             comment|               photo|         photo_after|              coords|             address|   subdistrict|         district|            province|           timestamp|state|star|count_reopen|       last_activity|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "|2021-FYJTFP|  ความสะอาด|          เขตบางซื่อ|             ขยะเยอะ|https://storage.g...|                null|[13.81865, 100.53...|12/14 ถนน กรุงเทพ...|          null|             null|       กรุงเทพมหานคร|2021-09-03 12:51:...|    1|null|        null|2022-06-04 15:34:...|\n",
      "|2021-8GKAR9|      สายไฟ|ยังไม่มีหน่วยงานร...|1. เถาวัลย์งอดบนส...|https://storage.g...|                null|[13.79704, 100.57...|335/31 ลาดพร้าว แ...|     สามเสนนอก|         ห้วยขวาง|จังหวัดกรุงเทพมหานคร|2021-09-19 06:47:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-AFPUXZ|        ถนน|                null|1 ซ่อมทางเท้าหลัง...|https://storage.g...|                null|[13.72338, 100.52...|37 10 ซอย สีลม 9 ...|          สีลม|           บางรัก|       กรุงเทพมหานคร|2021-09-19 07:40:...|    0|null|        null|2022-02-22 04:30:...|\n",
      "|2021-CGPMUN|    น้ำท่วม|เขตประเวศ,ฝ่ายโยธ...|น้ำท่วมเวลาฝนตกแล...|https://storage.g...|https://storage.g...|[13.67891, 100.66...|189 เฉลิมพระเกียร...|       หนองบอน|           ประเวศ|       กรุงเทพมหานคร|2021-09-19 14:56:...|    1|   4|        null|2022-06-21 08:21:...|\n",
      "|2021-8Z8JQ3|        ถนน|                null|1) พื้นที่นี้ขาดห...|https://storage.g...|                null|[13.72804, 100.86...|3/8 ขุมทอง-ลำต้อย...|        ขุมทอง|        ลาดกระบัง|       กรุงเทพมหานคร|2021-09-20 00:24:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-G4FN8W|       คลอง|                null|   น้ำเน่าเสียในคลอง|https://storage.g...|                null|[13.74227, 100.62...|50 พระราม ที่ 9 แ...|       สวนหลวง|          สวนหลวง|       กรุงเทพมหานคร|2021-09-21 05:22:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-AFJYNF|        ถนน|                null|น้ำท่วม เอ่อล้นถน...|https://storage.g...|                null|[13.73614, 100.71...|3 ถนน พัฒนาชนบท 3...|คลองสองต้นนุ่น|        ลาดกระบัง|       กรุงเทพมหานคร|2021-09-22 05:06:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-HAJULK|  ร้องเรียน|                null|มีการยกถนนในซอยเม...|https://storage.g...|                null|[13.79225, 100.71...|95 ซอย 12 มีนบุรี...|       มีนบุรี|          มีนบุรี|       กรุงเทพมหานคร|2021-09-23 06:25:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-7XATFA|      สะพาน|             เขตสาทร|สะพานลอยปรับปรุงไ...|https://storage.g...|                null|[13.72060, 100.52...|191/1 ถนน สาทรเหน...|       ยานนาวา|             สาทร|       กรุงเทพมหานคร|2021-09-26 05:03:...|    1|null|        null|2022-06-06 01:17:...|\n",
      "|2021-79YL3M|        ถนน|                null|1.ซอยราษฎร์นิมิตร...|https://storage.g...|                null|[13.91732, 100.73...|147 ซอย ราษฎร์นิม...| สามวาตะวันออก|        คลองสามวา|       กรุงเทพมหานคร|2021-09-28 23:01:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-9U2NJT|    น้ำท่วม|เขตบางซื่อ,ฝ่ายโย...|             น้ำท่วม|https://storage.g...|https://storage.g...|[13.81853, 100.53...|12/14 ถนน กรุงเทพ...|          null|             null|       กรุงเทพมหานคร|2021-10-14 10:45:...|    1|null|        null|2022-09-08 08:35:...|\n",
      "|2021-F7A2PK|ท่อระบายน้ำ|                null|มีฝาท่อระบายน้ำชำ...|https://storage.g...|                null|[13.71937, 100.44...|19/31 ซอย เพชรเกษ...|       บางหว้า|        ภาษีเจริญ|       กรุงเทพมหานคร|2021-11-12 02:19:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-DHG7W7|    ทางเท้า|                null|             ทางเท้า|https://storage.g...|                null|[13.75191, 100.53...|542/26 ถนน เพชรบุ...|      ถนนพญาไท|          ราชเทวี|       กรุงเทพมหานคร|2021-11-14 10:21:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-DANRZQ|    ทางเท้า|                null|             ทางเท้า|https://storage.g...|                null|[13.74463, 100.51...|326 ถนน พลับพลาไช...|      ป้อมปราบ|ป้อมปราบศัตรูพ่าย|       กรุงเทพมหานคร|2021-11-16 04:16:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-94U766|       คลอง|                null|มีผู้นำขยะมาแยกแล...|https://storage.g...|                null|[13.71290, 100.65...|17/1 ซอย อ่อนนุช ...|       สวนหลวง|          สวนหลวง|       กรุงเทพมหานคร|2021-11-25 03:43:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-EBHHRR|    น้ำท่วม|                null|น้ำท่วมขังเป็นสัป...|https://storage.g...|                null|[13.92362, 100.58...|79/182 ถ. ประชาอุ...|      ดอนเมือง|         ดอนเมือง|       กรุงเทพมหานคร|2021-12-09 11:06:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-3TFBXH|        ถนน|                null|อยากได้ไฟแสงสว่าง...|https://storage.g...|                null|[13.82401, 100.62...|144 หมู่ที่ 12 ถน...|      ลาดพร้าว|         ลาดพร้าว|       กรุงเทพมหานคร|2021-12-09 11:30:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-BX7JYW|        ถนน|                null|ถนนเจริญนคร บริเว...|https://storage.g...|                null|[13.70383, 100.48...|ตลาดจักรกล แขวง ส...|        สำเหร่|           ธนบุรี|       กรุงเทพมหานคร|2021-12-09 11:51:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-7C8DMQ|    น้ำท่วม|                null|ที่จอดรถทางเข้าสว...|https://storage.g...|                null|[13.73315, 100.53...|ประตูราชดำริ สวนล...|       ลุมพินี|          ปทุมวัน|       กรุงเทพมหานคร|2021-12-09 12:11:...|    0|null|        null|2022-02-22 04:29:...|\n",
      "|2021-DVEWYM|    น้ำท่วม|เขตลาดพร้าว,ฝ่ายโ...|ซอยลาดพร้าววังหิน...|https://storage.g...|                null|[13.82280, 100.59...|702 ถ. ลาดพร้าววั...|      ลาดพร้าว|         ลาดพร้าว|       กรุงเทพมหานคร|2021-12-09 12:29:...|    1|   5|        null|2022-08-12 07:18:...|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+--------------------+--------------------+-----+----+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_element = udf(lambda x: x[0], StringType())\n",
    "df_exploded = df_use.withColumn('type', first_element(df_use['type']))\n",
    "df_exploded = df_exploded.filter(\"type != ''\")\n",
    "\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_exploded.withColumn(\"latitude\", col(\"coords\")[0]) \\\n",
    "       .withColumn(\"longitude\", col(\"coords\")[1]) \\\n",
    "       .drop(\"coords\")\n",
    "df_t = df_t.withColumn(\"latitude\", col(\"latitude\").cast('double'))\n",
    "df_t = df_t.withColumn(\"longitude\", col(\"longitude\").cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ticket_id='จักขอบคุณอย่างยิ่ง\"', type='https://storage.googleapis.com/traffy_public_bucket/attachment/2022-02/5d20ecb864e916af948b4090ce9405be6f0b2f04.jpg', organization=None, comment='100.46930,13.75503', photo='1186 ถ. พรานนก แขวง บ้านช่างหล่อ เขตบางกอกน้อย กรุงเทพมหานคร 10700 ประเทศไทย', photo_after='บางขุนศรี', address='กรุงเทพมหานคร', subdistrict='2022-02-06 16:59:19.251528+00', district='กำลังดำเนินการ', province=None, timestamp=None, state=0, star=None, count_reopen=None, last_activity=None, latitude=None, longitude=None) \n",
      "\n",
      "Row(ticket_id='จักขอบคุณอย่างยิ่ง\"', type='https://storage.googleapis.com/traffy_public_bucket/attachment/2022-02/5d20ecb864e916af948b4090ce9405be6f0b2f04.jpg', organization=None, comment='100.46930,13.75503', photo='1186 ถ. พรานนก แขวง บ้านช่างหล่อ เขตบางกอกน้อย กรุงเทพมหานคร 10700 ประเทศไทย', photo_after='บางขุนศรี', address='กรุงเทพมหานคร', subdistrict='2022-02-06 16:59:19.251528+00', district='กำลังดำเนินการ', province=None, timestamp=None, state=0, star=None, count_reopen=None, last_activity=None, latitude=None, longitude=None) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_latitude = df_t.filter(df_t.latitude.isNull())\n",
    "null_longitude = df_t.filter(df_t.longitude.isNull())\n",
    "print(check_first_null(null_latitude), \"\\n\")\n",
    "print(check_first_null(null_longitude), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_t.dropna(how='all', subset=['latitude'])\n",
    "df_t = df_t.dropna(how='all', subset=['longitude'])\n",
    "df_t = df_t.dropna(how='all', subset=['subdistrict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_6f8215f805b0,\n",
       " OneHotEncoder_bb3d82c4692f,\n",
       " VectorAssembler_737f6f55c8fa]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "categoricalAttributes = ['type']\n",
    "for columnName in categoricalAttributes:\n",
    "    stringIndexer = StringIndexer(inputCol=columnName, outputCol=columnName+ \"Index\")\n",
    "    stages.append(stringIndexer)\n",
    "    oneHotEncoder = OneHotEncoder(inputCol=columnName+ \"Index\", outputCol=columnName + \"Vec\")\n",
    "    stages.append(oneHotEncoder)\n",
    "    \n",
    "categoricalCols = [s + \"Vec\" for s in categoricalAttributes]\n",
    "\n",
    "numericColumns = ['latitude', 'longitude']\n",
    "\n",
    "allFeatureCols = numericColumns + categoricalCols\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=allFeatureCols,\n",
    "    outputCol=\"features\")\n",
    "stages.append(vectorAssembler)\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_t.select(['type', 'subdistrict', 'latitude', 'longitude'])\n",
    "train_df, test_df = df_select.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurePipeline = Pipeline(stages=stages)\n",
    "featureOnlyModel = featurePipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+---------+---------+---------------+--------------------+\n",
      "| type|subdistrict|latitude|longitude|typeIndex|        typeVec|            features|\n",
      "+-----+-----------+--------+---------+---------+---------------+--------------------+\n",
      "|PM2.5| กระทุ่มราย|13.85535|100.86528|     17.0|(23,[17],[1.0])|(25,[0,1,19],[13....|\n",
      "+-----+-----------+--------+---------+---------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingFeaturesDf = featureOnlyModel.transform(train_df)\n",
    "testFeaturesDf = featureOnlyModel.transform(test_df)\n",
    "trainingFeaturesDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(25, {0: 13.8554, 1: 100.8653, 19: 1.0}), latitude=13.85535, longitude=100.86528),\n",
       " Row(features=SparseVector(25, {0: 13.8563, 1: 100.863, 19: 1.0}), latitude=13.85633, longitude=100.86304),\n",
       " Row(features=SparseVector(25, {0: 13.6974, 1: 100.8515, 19: 1.0}), latitude=13.69736, longitude=100.8515),\n",
       " Row(features=SparseVector(25, {0: 13.7018, 1: 100.8542, 19: 1.0}), latitude=13.70184, longitude=100.85423),\n",
       " Row(features=SparseVector(25, {0: 13.7547, 1: 100.8514, 19: 1.0}), latitude=13.75473, longitude=100.85137)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingFeaturesDf.select(\"features\", 'latitude', 'longitude').rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=100, maxDepth=5, labelCol='subdistrict', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol='type', featuresCol='features')\n",
    "dtPipeline = Pipeline(stages=[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column type must be of type numeric but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17916\\791832439.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdtPipelineModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtPipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingFeaturesDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column type must be of type numeric but was actually of type string."
     ]
    }
   ],
   "source": [
    "dtPipelineModel = dtPipeline.fit(trainingFeaturesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2828463697250392\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "type_indexer = StringIndexer(inputCol='type', outputCol='label')\n",
    "df_indexed = type_indexer.fit(df_t).transform(df_t)\n",
    "\n",
    "# Create a feature vector\n",
    "assembler = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features')\n",
    "df_features = assembler.transform(df_indexed)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = df_features.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf = RandomForestClassifier(numTrees=100, maxDepth=5, labelCol='label', featuresCol='features')\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Evaluate performance on test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "predictions = rf_model.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column subdistrict is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17916\\1376629963.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0massembler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'longitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'subdistrict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massembler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpreprocessed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Split data into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Data type string of column subdistrict is not supported."
     ]
    }
   ],
   "source": [
    "df_select = df_t.select(['type', 'latitude', 'longitude', 'subdistrict'])\n",
    "\n",
    "indexer = StringIndexer(inputCol='type', outputCol='label')\n",
    "\n",
    "subdistrict_indexer = StringIndexer(inputCol='subdistrict', outputCol='subdistrict'+'indexed')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['latitude', 'longitude', 'subdistrict'], outputCol='features')\n",
    "pipeline = Pipeline(stages=[indexer, assembler])\n",
    "preprocessed_data = pipeline.fit(df_select).transform(df_select)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features', maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2815206349752931\n"
     ]
    }
   ],
   "source": [
    "df_select = df_t.select(['type', 'latitude', 'longitude'])\n",
    "\n",
    "indexer = StringIndexer(inputCol='type', outputCol='label')\n",
    "assembler = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features')\n",
    "pipeline = Pipeline(stages=[indexer, assembler])\n",
    "preprocessed_data = pipeline.fit(df_select).transform(df_select)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features', maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringIndexer' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17916\\2454025851.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moneHotEncoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"subdistrict\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_index\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"subdistrict\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# feature_indexed = feature_indexer.fit(df_select).transform(df_select)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moneHotEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_indexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mfeature_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_indexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0massembler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"subdistrict\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_vec\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Acer\\miniconda3\\envs\\py3109\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StringIndexer' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "df_select = df_t.select(['type', 'subdistrict', 'latitude', 'longitude'])\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"type\", outputCol=\"label\").fit(df_select)\n",
    "df_select = label_indexer.transform(df_select)\n",
    "feature_indexer = StringIndexer(inputCol=\"subdistrict\", outputCol=\"features\").fit(df_select)\n",
    "df_select = feature_indexer.transform(df_select)\n",
    "# oneHotEncoder = OneHotEncoder(inputCol=\"subdistrict\" + \"_index\", outputCol=\"subdistrict\" + \"_vec\")\n",
    "# feature_indexed = feature_indexer.fit(df_select).transform(df_select)\n",
    "# ohe = oneHotEncoder.fit(feature_indexer)\n",
    "# feature_indexer = ohe.transform(feature_indexer)\n",
    "# assembler = VectorAssembler(inputCols=[\"subdistrict\" + \"_vec\"], outputCol='features')\n",
    "# df_select = assembler.transform(df_select)\n",
    "\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler])\n",
    "preprocessed_data = pipeline.fit(df_select).transform(df_select)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = df_select.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features', maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_t.select(['type', 'subdistrict', 'latitude', 'longitude'])\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"type\", outputCol=\"label\").fit(df_select)\n",
    "df_select = label_indexer.transform(df_select)\n",
    "feature_indexer = StringIndexer(inputCol=\"subdistrict\", outputCol=\"features\").fit(df_select)\n",
    "df_select = feature_indexer.transform(df_select)\n",
    "\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler])\n",
    "preprocessed_data = pipeline.fit(df_select).transform(df_select)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = df_select.randomSplit([0.7, 0.3], seed=112)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features', maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Python 3.10.9)",
   "language": "python",
   "name": "py3109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
